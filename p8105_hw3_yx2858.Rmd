---
title: "p8105_hw3_yx2858"
author: "Yueyi Xu"
date: "2023-10-08"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(p8105.datasets)
library(tidyverse)
library(dplyr)
```

# Problem 1
#### Question1_1
How many aisles are there, and which aisles are the most items ordered from?
```{r}
data("instacart") #import the data instacart
question1_1 <- instacart %>% #assign the variable
  count(aisle) %>% #count the number of aisle
  arrange(desc(n)) #arrange the aisle in descending order
```
There are 134 aisles, and fresh vegetables are the most items ordered from.


#### Question1_2
Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.
```{r}
question1_2 <- question1_1 %>% #assign the variable
  filter(n > 10000) %>% #filter the aisles with more than 10000 items ordered
  arrange(desc(n)) #arrange in descending order
ggplot(data = question1_2, aes(x = aisle, y = n, group = 1)) + #graph the plot
  geom_line() + #add the line
  geom_point() + #add the point
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) #reverse the names of x axis to horizontal view
```
There are 39 aisles with more than 10000 items ordered.


#### Question1_3
Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r}
instacart %>%
  filter(aisle %in% c('baking ingredients', 'dog food care', 'packaged vegetables fruits')) %>% #filter the dataset into only these three values
  group_by(aisle) %>% #group the filtered dataset
  count(product_name) %>% #count the number of each products
  top_n(3) #select the top 3 products with the highest count
```


#### Question1_4
Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
```{r}
instacart %>%
  filter(product_name %in% c('Pink Lady Apples', 'Coffee Ice Cream')) %>% #filter the dataset into which "product_name" is either "Pink Lady Apples" or "Coffee Ice Cream"
  group_by(product_name, order_dow) %>% #group the filtered dataset for each combination of product and day of the week
  summarize(Mean = mean(order_hour_of_day, na.rm = TRUE)) %>% #calculate the mean hour of the day when orders for each product were placed, ignore the ones with missing value
  pivot_wider(names_from = order_dow, values_from = Mean) #pivot the summarized dataset into each day of the week as a separate column with the mean order hour; therefore, this table becomes a 2x7 table (2 product_name, and 7 days of the week)
```



# Problem 2
#### Question2_1
Data Cleaning:
```{r}
data("brfss_smart2010")
question2_1 <- brfss_smart2010 %>% #assign the variable
  filter(Topic == 'Overall Health') %>% #filter the dataset to include only "Overall Health"
  group_by(Response) %>% #group the dataset by the "Response" variable
  mutate(Response = factor(Response, levels = c('Poor', 'Fair', 'Good', 'Very good', 'Excellent'))) %>% #modify the "Response" variable by converting it 
  ungroup(Response) #ungroup the uncleaned dataset
```


#### Question2_2
In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
question2_2_2002 <- question2_1 %>% #assign the variable
  filter(Year == 2002) %>% #filter the dataset to include only data for the year 2002 
  group_by(Locationabbr) %>% #group the dataset by the "Locationabbr" variable which represents the state
  summarize(count_locationdistinct = n_distinct(Locationdesc)) %>% #in each state group, calculate the number of distinct "Locationdesc" values and add up together
  filter(count_locationdistinct >= 7) #filter the result to include only states where the count of distinct "Locationdesc" values is greater or equal to 7
question2_2_2010 <- question2_1 %>% #assign the variable
  filter(Year == 2010) %>% #filter the dataset to include only data for the year 2010
  group_by(Locationabbr) %>% #group the dataset by the "Locationabbr" variable which represents the state
  summarize(count_locationdistinct = n_distinct(Locationdesc)) %>% #in each state group, calculate the number of distinct "Locationdesc" values and add up together
  filter(count_locationdistinct >= 7) #filter the result to include only states where the count of distinct "Locationdesc" values is greater or equal to 7
```
In 2002, CT, FL, MA, NC, NJ, and PA were observed at 7 or more locations. In 2010, CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA were observed at 7 or more locations.


#### Question2_3
Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).
```{r}
question2_3_lineplot <- question2_1 %>% #assign a line plot variable
  filter(Response == 'Excellent') %>% #filter and limit to only excellent response
  select(Year, Locationabbr, Data_value) %>% #select columns from the filtered dataset of "Year", "Locationabbr", and "Data_value" 
  group_by(Year, Locationabbr) %>% #group the dataset of "Year" and "Locationabbr"
  summarize(Data_value = mean(Data_value, na.rm =TRUE)) %>% #calculate the mean of "Data_value" for each of "Year" and "Locationabbr", ignore the ones with missing value
  ungroup(Year, Locationabbr) #ungroup the uncleaned dataset
ggplot(data = question2_3_lineplot, aes(x = Year, y = Data_value)) +
  geom_line(aes(color = Locationabbr)) #set up the dataset of "question2_1_lineplot" and map the x axis as the "Year" variable and y axis as the "Data_value" variable; add a line to create a line chart and set each of the "Locationabbr" with a different color in the plot
```


