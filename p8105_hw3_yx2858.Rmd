---
title: "p8105_hw3_yx2858"
author: "Yueyi Xu"
date: "2023-10-08"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(p8105.datasets)
library(tidyverse)
library(dplyr)
```

# Problem 1
#### Question1_1
How many aisles are there, and which aisles are the most items ordered from?
```{r}
data("instacart") #import the data instacart
instacart = instacart |> #assign the variable
  as_tibble()
```

```{r}
instacart |>
  count(aisle) |> #count the number of aisle
  arrange(desc(n)) #arrange the aisle in descending order
```
There are 134 aisles, and fresh vegetables are the most items ordered from.


#### Question1_2
Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.
```{r}
instacart |> 
  count(aisle) |> 
  filter(n > 10000) |> #filter the aisles with more than 10000 items ordered
  mutate(aisle = fct_reorder(aisle, n)) |> 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) #reverse the names of x axis to horizontal view
```
There are 39 aisles with more than 10000 items ordered.


#### Question1_3
Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r}
instacart |> 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) |> #filter the dataset into only these three values
  group_by(aisle) |> #group the filtered dataset
  count(product_name) |> #count the number of each products
  mutate(rank = min_rank(desc(n))) |> 
  filter(rank < 4) |> #select the top 3 products with the highest count
  arrange(desc(n)) |>
  knitr::kable()
```


#### Question1_4
Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
```{r}
instacart |>
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |> #filter the dataset into which "product_name" is either "Pink Lady Apples" or "Coffee Ice Cream"
  group_by(product_name, order_dow) |> #group the filtered dataset for each combination of product and day of the week
  summarize(mean_hour = mean(order_hour_of_day)) |> #calculate the mean hour of the day when orders for each product were placed, ignore the ones with missing value
  pivot_wider( #pivot the summarized dataset into each day of the week as a separate column with the mean order hour; therefore, this table becomes a 2x7 table (2 product_name, and 7 days of the week)
    names_from = order_dow, 
    values_from = mean_hour) |>
  knitr::kable(digits = 2)
```



# Problem 2
```{r}
data("brfss_smart2010")
```

#### Question2_1
Data Cleaning:
```{r}
brfss_clean_df = 
  brfss_smart2010 |>
  janitor::clean_names() |>
  filter(topic == "Overall Health") |> #filter the dataset to include only "Overall Health"
  filter(response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") |>
  mutate(response = as.factor(response), 
         response = ordered(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))) #modify the "Response" variable by converting it 
```


#### Question2_2
In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
question2_2_2002 =
  filter(brfss_clean_df, year == 2002) |> #filter the dataset to include only data for the year 2002
  group_by(locationabbr) |> #group the dataset by the "locationabbr" variable which represents the state
  summarize(count_locationdistinct = n_distinct(locationdesc)) |> #in each state group, calculate the number of distinct "locationdesc" values and add up together
  filter(count_locationdistinct >= 7) #filter the result to include only states where the count of distinct "locationdesc" values is greater or equal to 7
locationabbr_2002 = #extract "locationabbr" column from the "locationabbr_2002" dataset
  pull(question2_2_2002, locationabbr)

question2_2_2010 =
  filter(brfss_clean_df, year == 2010) |> #filter the dataset to include only data for the year 2002
  group_by(locationabbr) |> #group the dataset by the "locationabbr" variable which represents the state
  summarize(count_locationdistinct = n_distinct(locationdesc)) |> #in each state group, calculate the number of distinct "locationdesc" values and add up together
  filter(count_locationdistinct >= 7) #filter the result to include only states where the count of distinct "locationdesc" values is greater or equal to 7
locationabbr_2010 = #extract "locationabbr" column from the "locationabbr_2010" dataset
  pull(question2_2_2010, locationabbr)
```

In 2002, `r locationabbr_2002` were observed at 7 or more locations. In 2010, `r locationabbr_2010` were observed at 7 or more locations.


#### Question2_3
Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).
```{r}
question2_3_excellent =
  brfss_clean_df |>
  filter(response == "Excellent") |> #filter and limit to only excellent response
  group_by(year, locationabbr) |> #group the dataset of "year" and "locationabbr"
  summarize(average_data = mean(data_value, na.rm = TRUE)) #calculate the mean of "data_value" for each of "year" and "locationabbr", ignore the ones with missing value

question2_3_excellent |>
  ggplot(aes(x = year, y = average_data, color = locationabbr)) +
  geom_path() +
  labs( #set up the dataset of "question2_3_excellent" and map the x axis as the "Year" variable and y axis as the "Average_value" variable
    title = "Average value over time within a state",
    x = "Year",
    y = "Average value",
    caption = "Data from the p8105.datasets"
  ) +
  theme_minimal() #apply the minimalistic theme to the plot, result in a clean and minimal appearance
```


#### Question2_4
Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.
```{r}
question2_4_2006_2010 = 
  brfss_clean_df |>
  filter(locationabbr == "NY") |> #filter and limit to only "NY" state
  filter(year == 2006 | year == 2010) |> #filter to only years 2006 and 2010
  select(year, locationabbr, locationdesc, response, data_value) |> #select the specific columns from the filtered data
  group_by(year, locationabbr) #group the data by "year" and "locationabbr"

question2_4_2006_2010 |>
  ggplot(aes(x = response, y = data_value)) + #specify the ggplot object with x axis of response and y axis of data_value
  geom_boxplot() + #add boxplot to the plot
  facet_wrap(~ year, ncol = 2) + #split the plot into smaller subplots based on the "year" variable and arrange them in 2 columns; create separate boxplots for 2006 and 2010
  labs( #set the boxplot's title, x axis label, y axis label, and caption
    title = "Distribution of data_value for responses among locations in NY State in 2006 and 2010",
    x = "Year",
    y = "Distribution of Data_value",
    caption = "Data from the p8105.datasets"
  )
```



# Problem 3
Clean, organize the nhanes_covar dataset:
```{r}
covar_clean_df =
  read_csv("nhanes_covar.csv", skip = 4) |> #import the data to dataframe
  janitor::clean_names() |> #clean the names
  filter(age >= 21) |> #filter the data to include only age larger than 21
  mutate(
    sex = replace(sex, sex == 1, "Male"),
    sex = replace(sex, sex == 2, "Female"),
    education = replace(education, education == 1, "Less than high school"),
    education = replace(education, education == 2, "High school equivalent"),
    education = replace(education, education == 3, "More than high school")
  ) |> #replace the values in sex and education columns
  mutate(sex = as.factor(sex),
         education = as.factor(education)) |> #convert the sex and education columns to factor variables
  na.omit() #remove rows with missing values from the dataset
```


Clean, organize the nhanes_accel dataset:
```{r}
accel_clean_df =
  read_csv("nhanes_accel.csv") |> #import the data to dataframe
  janitor::clean_names() |> #clean the names
  pivot_longer(
    min1:min1440, #specify the range of columns to pivot 
    names_to = "number", #rename the columns of "number" to represent "min"
    values_to = "counts", #rename the columns of "counts" to represent "values" in "min"
    names_prefix = "min" #remove the "min" in front of all column names
  )
```


Combine two dataframes:
```{r}
combine_covar_accel = #create a combined dataframe
  left_join(covar_clean_df, accel_clean_df, by = "seqn") #left_join combine two dataframes based on the common column "seqn"
```


#### Question3_1
Produce a reader-friendly table for the number of men and women in each education category, and create a visualization of the age distributions for men and women in each education category. Comment on these items.
```{r}
sex_education = 
  select(covar_clean_df, sex | education) #select only sex and education columns from the dataframe
table(sex_education) #generate a table containing the numbers of sex and education columns only
```


