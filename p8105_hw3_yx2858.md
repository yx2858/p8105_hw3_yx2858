p8105_hw3_yx2858
================
Yueyi Xu
2023-10-08

``` r
library(p8105.datasets)
library(tidyverse)
library(dplyr)
```

# Problem 1

#### Question1_1

How many aisles are there, and which aisles are the most items ordered
from?

``` r
data("instacart") #import the data instacart
instacart = instacart |> #assign the variable
  as_tibble()
```

``` r
instacart |>
  count(aisle) |> #count the number of aisle
  arrange(desc(n)) #arrange the aisle in descending order
```

    ## # A tibble: 134 × 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ℹ 124 more rows

There are 134 aisles, and fresh vegetables are the most items ordered
from.

#### Question1_2

Make a plot that shows the number of items ordered in each aisle,
limiting this to aisles with more than 10000 items ordered. Arrange
aisles sensibly, and organize your plot so others can read it.

``` r
instacart |> 
  count(aisle) |> 
  filter(n > 10000) |> #filter the aisles with more than 10000 items ordered
  mutate(aisle = fct_reorder(aisle, n)) |> 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) #reverse the names of x axis to horizontal view
```

![](p8105_hw3_yx2858_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->
There are 39 aisles with more than 10000 items ordered.

#### Question1_3

Make a table showing the three most popular items in each of the aisles
“baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
Include the number of times each item is ordered in your table.

``` r
instacart |> 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) |> #filter the dataset into only these three values
  group_by(aisle) |> #group the filtered dataset
  count(product_name) |> #count the number of each products
  mutate(rank = min_rank(desc(n))) |> 
  filter(rank < 4) |> #select the top 3 products with the highest count
  arrange(desc(n)) |>
  knitr::kable()
```

| aisle                      | product_name                                  |    n | rank |
|:---------------------------|:----------------------------------------------|-----:|-----:|
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |

#### Question1_4

Make a table showing the mean hour of the day at which Pink Lady Apples
and Coffee Ice Cream are ordered on each day of the week; format this
table for human readers (i.e. produce a 2 x 7 table).

``` r
instacart |>
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |> #filter the dataset into which "product_name" is either "Pink Lady Apples" or "Coffee Ice Cream"
  group_by(product_name, order_dow) |> #group the filtered dataset for each combination of product and day of the week
  summarize(mean_hour = mean(order_hour_of_day)) |> #calculate the mean hour of the day when orders for each product were placed, ignore the ones with missing value
  pivot_wider( #pivot the summarized dataset into each day of the week as a separate column with the mean order hour; therefore, this table becomes a 2x7 table (2 product_name, and 7 days of the week)
    names_from = order_dow, 
    values_from = mean_hour) |>
  knitr::kable(digits = 2)
```

    ## `summarise()` has grouped output by 'product_name'. You can override using the
    ## `.groups` argument.

| product_name     |     0 |     1 |     2 |     3 |     4 |     5 |     6 |
|:-----------------|------:|------:|------:|------:|------:|------:|------:|
| Coffee Ice Cream | 13.77 | 14.32 | 15.38 | 15.32 | 15.22 | 12.26 | 13.83 |
| Pink Lady Apples | 13.44 | 11.36 | 11.70 | 14.25 | 11.55 | 12.78 | 11.94 |

# Problem 2

``` r
data("brfss_smart2010")
```

#### Question2_1

Data Cleaning:

``` r
brfss_clean_df = 
  brfss_smart2010 |>
  janitor::clean_names() |>
  filter(topic == "Overall Health") |> #filter the dataset to include only "Overall Health"
  filter(response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") |>
  mutate(response = as.factor(response), 
         response = ordered(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))) #modify the "Response" variable by converting it 
```

#### Question2_2

In 2002, which states were observed at 7 or more locations? What about
in 2010?

``` r
question2_2_2002 =
  filter(brfss_clean_df, year == 2002) |> #filter the dataset to include only data for the year 2002
  group_by(locationabbr) |> #group the dataset by the "locationabbr" variable which represents the state
  summarize(count_locationdistinct = n_distinct(locationdesc)) |> #in each state group, calculate the number of distinct "locationdesc" values and add up together
  filter(count_locationdistinct >= 7) #filter the result to include only states where the count of distinct "locationdesc" values is greater or equal to 7
locationabbr_2002 = #extract "locationabbr" column from the "locationabbr_2002" dataset
  pull(question2_2_2002, locationabbr)

question2_2_2010 =
  filter(brfss_clean_df, year == 2010) |> #filter the dataset to include only data for the year 2002
  group_by(locationabbr) |> #group the dataset by the "locationabbr" variable which represents the state
  summarize(count_locationdistinct = n_distinct(locationdesc)) |> #in each state group, calculate the number of distinct "locationdesc" values and add up together
  filter(count_locationdistinct >= 7) #filter the result to include only states where the count of distinct "locationdesc" values is greater or equal to 7
locationabbr_2010 = #extract "locationabbr" column from the "locationabbr_2010" dataset
  pull(question2_2_2010, locationabbr)
```

In 2002, CT, FL, MA, NC, NJ, PA were observed at 7 or more locations. In
2010, CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, WA were
observed at 7 or more locations.

#### Question2_3

Construct a dataset that is limited to Excellent responses, and
contains, year, state, and a variable that averages the data_value
across locations within a state. Make a “spaghetti” plot of this average
value over time within a state (that is, make a plot showing a line for
each state across years – the geom_line geometry and group aesthetic
will help).

``` r
question2_3_excellent =
  brfss_clean_df |>
  filter(response == "Excellent") |> #filter and limit to only excellent response
  group_by(year, locationabbr) |> #group the dataset of "year" and "locationabbr"
  summarize(average_data = mean(data_value, na.rm = TRUE)) #calculate the mean of "data_value" for each of "year" and "locationabbr", ignore the ones with missing value
```

    ## `summarise()` has grouped output by 'year'. You can override using the
    ## `.groups` argument.

``` r
question2_3_excellent |>
  ggplot(aes(x = year, y = average_data, color = locationabbr)) +
  geom_path() +
  labs( #set up the dataset of "question2_3_excellent" and map the x axis as the "Year" variable and y axis as the "Average_value" variable
    title = "Average value over time within a state",
    x = "Year",
    y = "Average value",
    caption = "Data from the p8105.datasets"
  ) +
  theme_minimal() #apply the minimalistic theme to the plot, result in a clean and minimal appearance
```

![](p8105_hw3_yx2858_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

#### Question2_4

Make a two-panel plot showing, for the years 2006, and 2010,
distribution of data_value for responses (“Poor” to “Excellent”) among
locations in NY State.

``` r
question2_4_2006_2010 = 
  brfss_clean_df |>
  filter(locationabbr == "NY") |> #filter and limit to only "NY" state
  filter(year == 2006 | year == 2010) |> #filter to only years 2006 and 2010
  select(year, locationabbr, locationdesc, response, data_value) |> #select the specific columns from the filtered data
  group_by(year, locationabbr) #group the data by "year" and "locationabbr"

question2_4_2006_2010 |>
  ggplot(aes(x = response, y = data_value)) + #specify the ggplot object with x axis of response and y axis of data_value
  geom_boxplot() + #add boxplot to the plot
  facet_wrap(~ year, ncol = 2) + #split the plot into smaller subplots based on the "year" variable and arrange them in 2 columns; create separate boxplots for 2006 and 2010
  labs( #set the boxplot's title, x axis label, y axis label, and caption
    title = "Distribution of data_value for responses among locations in NY State in 2006 and 2010",
    x = "Year",
    y = "Distribution of Data_value",
    caption = "Data from the p8105.datasets"
  )
```

![](p8105_hw3_yx2858_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->
